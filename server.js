// server.js

import 'dotenv/config';
import express from 'express';
import { AgentExecutor } from './src/agentExecutor.js';
import { createOpenAIStreamChunk, createFinalStreamChunk, STREAM_DONE_CHUNK } from './src/streamUtils.js';

// --- é…ç½® ---
const app = express();
app.use(express.json());

const PORT = process.env.LLM_EXTEND_SERVE_PORT || 12566;
const REMOTE_LLM_ENDPOINT = process.env.LLM_EXTEND_REMOTE_ENDPOINT || "http://xxx.com/v1";
const API_KEY = process.env.LLM_EXTEND_REMOTE_API_KEY || "sk-114514";
const REMOTE_LLM_ID = process.env.LLM_EXTEND_REMOTE_LLM_ID || "Qwen3-235B-A22B-AWQ";
console.log(PORT, REMOTE_LLM_ENDPOINT, API_KEY, REMOTE_LLM_ID);

if (!API_KEY) {
    throw new Error("è¯·è®¾ç½®çŽ¯å¢ƒå˜é‡ LLM_EXTEND_ORIGIN_API_KEY");
}

// å®žä¾‹åŒ– Agent æ‰§è¡Œå™¨
const agentExecutor = new AgentExecutor(API_KEY, REMOTE_LLM_ENDPOINT);

app.get('/v1/models', (req, res) => {
    console.log("[Request] Received request for /v1/models");

    // æˆ‘ä»¬å¯ä»¥è¿”å›žä¸€ä¸ªæˆ‘ä»¬â€œæ”¯æŒâ€çš„æ¨¡åž‹åˆ—è¡¨ã€‚
    // å…³é”®åœ¨äºŽï¼Œè¿™äº›æ¨¡åž‹IDåœ¨åŽç»­çš„ /v1/chat/completions è¯·æ±‚ä¸­éƒ½ä¼šè¢«æˆ‘ä»¬çš„ä»£ç†å¤„ç†ã€‚
    // æˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰è¯·æ±‚éƒ½æŒ‡å‘æˆ‘ä»¬çœŸæ­£çš„åŽç«¯æ¨¡åž‹ã€‚
    const modelsList = [
        {
            // è¿™æ˜¯ä½ çœŸæ­£çš„åŽç«¯æ¨¡åž‹ï¼Œå»ºè®®æ”¾åœ¨ç¬¬ä¸€ä¸ª
            id: REMOTE_LLM_ID,
            object: "model",
            created: Math.floor(Date.now() / 1000),
            owned_by: "mcp-proxy-server",
        },
        {
            // è¿™æ˜¯ä¸€ä¸ªâ€œåˆ«åâ€æˆ–â€œå…¼å®¹åâ€ï¼Œä¸ºäº†è®©æ›´å¤šå®¢æˆ·ç«¯é»˜è®¤å°±èƒ½å·¥ä½œ
            // å®¢æˆ·ç«¯è¯·æ±‚ç”¨è¿™ä¸ªIDï¼Œæˆ‘ä»¬çš„æœåŠ¡å™¨å†…éƒ¨ä¼šæŠŠå®ƒå½“ä½œé»˜è®¤æ¨¡åž‹å¤„ç†
            id: "gpt-3.5-turbo",
            object: "model",
            created: Math.floor(Date.now() / 1000),
            owned_by: "mcp-proxy-server",
        },
        {
            // ä½ å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šåˆ«å
            id: "gpt-4",
            object: "model",
            created: Math.floor(Date.now() / 1000),
            owned_by: "mcp-proxy-server",
        }
    ];

    res.json({
        object: "list",
        data: modelsList,
    });
});

// --- API ç«¯ç‚¹ ---
app.post('/v1/chat/completions', async (req, res) => {
    const { model, messages, stream } = req.body;

    if (!stream) {
        // éžæµå¼å¤„ç†
        try {
            const finalAnswer = await agentExecutor.run({ model, messages });
            res.json({
                choices: [{ message: { role: 'assistant', content: finalAnswer } }],
            });
        } catch (error) {
            console.error(error);
            res.status(500).json({ error: "Agent execution failed." });
        }
        return;
    }
    
    // æµå¼å¤„ç†
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.flushHeaders();

    try {
        // Agent å†…éƒ¨æ˜¯éžæµå¼çš„ï¼Œå®ƒä¼šå®Œæ•´è¿è¡Œç›´åˆ°å¾—åˆ°æœ€ç»ˆç­”æ¡ˆ
        const finalAnswer = await agentExecutor.run({ model, messages });
        
        // å¾—åˆ°æœ€ç»ˆç­”æ¡ˆåŽï¼Œæˆ‘ä»¬å°†å…¶æ¨¡æ‹Ÿæˆæµå¼è¾“å‡ºè¿”å›žç»™å®¢æˆ·ç«¯
        // è¿™å¯ä»¥è®©å®¢æˆ·ç«¯æ— ç¼é›†æˆï¼Œå³ä½¿æˆ‘ä»¬å†…éƒ¨æ˜¯æ‰¹å¤„ç†
        
        // ç®€å•çš„åˆ†å—å‘é€
        const words = finalAnswer.split(/\s+/);
        for (const word of words) {
            const chunk = createOpenAIStreamChunk(model, word + ' ');
            res.write(chunk);
            // æ¨¡æ‹Ÿæ‰“å­—æ•ˆæžœ
            await new Promise(resolve => setTimeout(resolve, 50));
        }

    } catch (error) {
        console.error("[Server Error]", error);
        const errorChunk = createOpenAIStreamChunk(model, `Error: Agent failed to execute. ${error.message}`, 'stop');
        res.write(errorChunk);
    } finally {
        // å‘é€ç»“æŸä¿¡å·
        res.write(STREAM_DONE_CHUNK);
        res.end();
    }
});

app.listen(PORT, () => {
    console.log(`ðŸš€ MCP Proxy Server is running on http://localhost:${PORT}`);
});
